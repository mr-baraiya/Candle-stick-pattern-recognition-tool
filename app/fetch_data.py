import pandas as pd
import os
import pickle

# Cache directories
CACHE_DIR = os.path.join(os.path.dirname(__file__), 'cache')
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# In-memory caches
_memory_cache = {}   # full timeframe data
_date_cache = {}     # filtered ranges

def fetch_data(csv_file, start_date, end_date, timeframe='1min'):
    """
    Fetch data with:
    âœ… Disk cache for full timeframe
    âœ… Memory cache for full timeframe
    âœ… Date range cache with sub-range optimization
    """
    print(f"ğŸ” fetch_data called: {csv_file}, {start_date} to {end_date}, {timeframe}")
    
    key = f"{csv_file}_{timeframe}"
    range_key = f"{key}_{start_date}_{end_date}"

    # âœ… Exact match in cache
    if range_key in _date_cache:
        print(f"âš¡ Using exact date-range cache for {range_key}")
        return _date_cache[range_key]

    # âœ… Check if a parent range exists (bigger range already cached)
    parent_key = None
    for cached_range in _date_cache.keys():
        if cached_range.startswith(key):
            parts = cached_range.split('_')
            if len(parts) >= 4:
                start, end = parts[-2], parts[-1]
                if start <= start_date and end >= end_date:
                    parent_key = cached_range
                    break

    if parent_key:
        print(f"âš¡ Using parent cache {parent_key} and slicing for new range")
        parent_data = _date_cache[parent_key]
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
        
        filtered = parent_data[
            (parent_data['datetime'] >= start_dt) &
            (parent_data['datetime'] <= end_dt)
        ].copy()
        _date_cache[range_key] = filtered
        return filtered

    # âœ… Load full timeframe data from memory or disk
    if key in _memory_cache:
        print(f"âš¡ Using in-memory cache for {key}")
        data = _memory_cache[key]
    else:
        data = _load_from_disk_or_csv(csv_file, timeframe)
        if data.empty:
            print(f"âŒ No data loaded for {csv_file}")
            return pd.DataFrame()
        _memory_cache[key] = data

    # âœ… Slice data for requested range
    start_datetime = pd.to_datetime(start_date)
    end_datetime = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)  # Include full end date

    if data.empty:
        print(f"âŒ Data is empty for {csv_file}")
        return pd.DataFrame()

    # Ensure datetime is the index
    if 'datetime' in data.columns:
        data = data.set_index('datetime')
    elif data.index.name != 'datetime':
        print(f"âŒ No datetime index found in data for {csv_file}")
        return pd.DataFrame()

    print(f"ğŸ“… Slicing data from {start_datetime} to {end_datetime}")
    print(f"ğŸ“Š Available data range: {data.index.min()} to {data.index.max()}")
    
    # Filter data using boolean indexing for better compatibility
    filtered = data[(data.index >= start_datetime) & (data.index <= end_datetime)].copy()
    filtered.reset_index(inplace=True)
    
    print(f"âœ… Filtered data: {len(filtered)} rows")

    # âœ… Cache this range for next time
    _date_cache[range_key] = filtered
    return filtered


def _load_from_disk_or_csv(csv_file, timeframe):
    """Load full data from disk cache or CSV."""
    data_dir = os.path.join(os.path.dirname(__file__), 'Data')
    csv_path = os.path.join(data_dir, csv_file)

    if not os.path.exists(csv_path):
        print(f"âŒ CSV file not found: {csv_path}")
        return pd.DataFrame()

    cache_meta = os.path.join(CACHE_DIR, f"{csv_file}.meta")
    cache_file = os.path.join(CACHE_DIR, f"{csv_file}_{timeframe}.pkl")
    current_time = str(os.path.getmtime(csv_path))

    # âœ… If disk cache is valid, load from it
    if os.path.exists(cache_file) and os.path.exists(cache_meta):
        with open(cache_meta, 'r') as meta:
            cached_time = meta.read().strip()
        if cached_time == current_time:
            print(f"âœ… Loaded {csv_file} [{timeframe}] from disk cache")
            with open(cache_file, 'rb') as f:
                return pickle.load(f)

    # âœ… Otherwise, load CSV and cache it
    return _load_and_cache(csv_path, cache_meta, timeframe, cache_file)


def _load_and_cache(csv_path, cache_meta, timeframe, cache_file):
    """Load CSV, aggregate if needed, and cache to disk."""
    print(f"ğŸ”„ Loading {os.path.basename(csv_path)} for {timeframe} and caching...")
    
    try:
        df = pd.read_csv(csv_path)
        print(f"ğŸ“Š Loaded CSV with {len(df)} rows and columns: {list(df.columns)}")
        
        # Check if required columns exist
        if 'date' not in df.columns or 'time' not in df.columns:
            print(f"âŒ Required columns (date, time) not found in CSV. Available columns: {list(df.columns)}")
            return pd.DataFrame()
        
        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])
        df = df.set_index('datetime')
        print(f"âœ… Created datetime index. Date range: {df.index.min()} to {df.index.max()}")

        if timeframe != '1min':
            df = aggregate_timeframe(df, timeframe)
            print(f"ğŸ“ˆ Aggregated to {timeframe}: {len(df)} rows")

        with open(cache_file, 'wb') as f:
            pickle.dump(df, f)
        with open(cache_meta, 'w') as meta:
            meta.write(str(os.path.getmtime(csv_path)))
        
        print(f"ğŸ’¾ Cached data to {cache_file}")
        return df
        
    except Exception as e:
        print(f"âŒ Error loading CSV {csv_path}: {str(e)}")
        return pd.DataFrame()


def aggregate_timeframe(data, timeframe):
    """Aggregate data into different timeframes."""
    timeframe_map = {
        '1min': '1T',
        '5min': '5T',
        '15min': '15T',
        '30min': '30T',
        '1hour': '1H'
    }
    
    if timeframe not in timeframe_map:
        print(f"âŒ Invalid timeframe: {timeframe}")
        return data
    
    print(f"ğŸ”„ Aggregating data to {timeframe} using rule '{timeframe_map[timeframe]}'")
    
    # Ensure data has the required columns
    required_columns = ['open', 'high', 'low', 'close', 'volume']
    missing_columns = [col for col in required_columns if col not in data.columns]
    if missing_columns:
        print(f"âŒ Missing columns for aggregation: {missing_columns}")
        return data
    
    try:
        resampled = data.resample(timeframe_map[timeframe]).agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna()
        
        print(f"âœ… Aggregated from {len(data)} to {len(resampled)} rows")
        return resampled
        
    except Exception as e:
        print(f"âŒ Error during aggregation: {str(e)}")
        return data


def get_available_csv_files():
    """Return all available CSV files."""
    data_dir = os.path.join(os.path.dirname(__file__), 'Data')
    if os.path.exists(data_dir):
        csv_files = [file for file in os.listdir(data_dir) if file.endswith('.csv')]
        print(f"ğŸ“ Found {len(csv_files)} CSV files: {csv_files}")
        return csv_files
    else:
        print(f"âŒ Data directory not found: {data_dir}")
        return []


def get_available_timeframes():
    return [
        ('1min', '1 Minute'),
        ('5min', '5 Minutes'),
        ('15min', '15 Minutes'),
        ('30min', '30 Minutes'),
        ('1hour', '1 Hour')
    ]
